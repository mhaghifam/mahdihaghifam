# jemdoc: menu{MENU}{index.html}, nofooter  
==Mahdi Haghifam 

~~~
{}{img_left}{photo/photo.jpg}{alt text}{240}{203}
Ph.D. candidate,\n 
[https://www.ece.utoronto.ca/ University of Toronto‬], [https://vectorinstitute.ai/ Vector Institute‬]\n
[https://scholar.google.ca/citations?user=WADwBkkAAAAJ&hl=en \[Google Scholar\]] [https://github.com/mhaghifam \[Github\]] [https://twitter.com/HaghifamMahdi \[Twitter\]] [https://www.linkedin.com/in/mahdi-haghifam/ \[Linkedin\]] \n
Email: [mahdi.haghifam@mail.utoronto.ca mahdi.haghifam@mail.utoronto.ca]
\n
\n
\n
The beginning of knowledge is the discovery of something we do not understand. \n
— Frank Herbert
~~~


== About Me
I am a final year PhD candidate at University of Toronto and a graduate student researcher at Vector Institute‬. I am honored to be advised by  [http://danroy.org/ Prof. ‪Daniel M. Roy‬]. 
I also work closely with  [https://gkdz.org/ Dr. Gintare Karolina Dziugaite‬]. I received my B.Sc. and M.Sc. degrees both in Electrical Engineering from Sharif University of Technology.\n
Previously, I was a research intern at Google Brain where I was extremely lucky to be mentored by [http://www.thomas-steinke.net/ Dr. Thomas Steinke]  and [https://athakurta.squarespace.com/ Dr. Abhradeep Guha Thakurta‬] during Summer and Fall 2022‪. I was also a research intern at [https://www.elementai.com/ Element AI‬] in Winter 2019 and Fall 2020. In early 2020, I was a visiting student at  [https://www.ias.edu/ Institute of Advanced Study‬ (IAS)]  for special-year program on Optimization, Statistics, and Theoretical Machine Learning. 

* I will be joining Khoury College of Computer Sciences at Northeastern University as a Distinguished Postdoc Fellow in Fall 2023.*


== Research Interests
My research focuses broadly on statistical learning theory and Differential Privacy. I have been working on several areas of Generalization Theory in Machine Learning  with a focus on deriving provable guarantees for Machine Learning methods using information-theoretic tools. 
I am also interested in statistical inference and learning under privacy constraints. For a complete list of my publications, please visit the [publication.html Publications] page.

I like collaborations; feel free to reach out if you find common interests.
== Some Recent Papers
- A. Ganesh, M. Haghifam, T. Steinke, A. Thakurta $(\alpha\beta)$ `` Faster Differentially Private Convex Optimization via Second-Order Methods'', preprint, 2023 [https://arxiv.org/abs/2305.13209 \[paper\]].
- M. Haghifam\*, B. Rodriguez-Galvez\*, R. Thobaben, M. Skoglund, D. M. Roy, G. K. Dziugaite  ``Limitations of Information-Theoretic Generalization Bounds for Gradient Descent Methods in Stochastic Convex Optimization'', ALT 2023 [https://arxiv.org/abs/2212.13556 \[paper\]].
- M. Haghifam, G. K. Dziugaite, S. Moran,  D. M. Roy, ``Towards a Unified Information--Theoretic Framework for Generalization'', NeurIPS 2021  ({{<font color=red size=+0.5><b>}}Spotlight, <3% of submissions){{</b></font>}}
[https://arxiv.org/pdf/2111.05275.pdf \[paper\]].

