<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Mahdi Haghifam </title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="cv.pdf">CV</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Mahdi Haghifam </h1>
</div>
<table class="imgtable"><tr><td>
<img src="photo/photo.jpg" alt="alt text" width="240px" height="203px" />&nbsp;</td>
<td align="left"><p>Research Assistant Professor,<br /> 
Toyota Technological Institute at Chicago<br /> 
<a href="https://scholar.google.ca/citations?user=WADwBkkAAAAJ&amp;hl=en">[Google Scholar]</a>  <a href="https://twitter.com/HaghifamMahdi">[Twitter]</a> <a href="https://www.linkedin.com/in/mahdi-haghifam/">[Linkedin]</a> <br />
Email(preferred): <a href="mailto:haghifam.mahdi@gmail.com">haghifam.mahdi@gmail.com</a> <br />
Email: <a href="mailto:mhaghifam@ttic.edu">mhaghifam@ttic.edu</a> <br />
 <font color="red">Note: I don't have access to my northeastern email anymore.</font>
<br />
<br /></p>
</td></tr></table>
<h2>About Me</h2>
<p>I am a <a href="https://www.ttic.edu/what-is-a-research-assistant-professor/">‪research assistant professor</a> at the <a href="https://www.ttic.edu/">Toyota Technological Institute at Chicago (TTIC)‬</a>.  I was previously a  <a href="https://shorturl.at/kpquU">Distinguished Postdoctoral Researcher</a> at Khoury College of Computer Sciences at Northeastern University, fortunate to be working  <a href="https://jonathan-ullman.github.io/">Jonathan Ullman‬</a> and <a href="https://www.bu.edu/cs/profiles/adam-smith/">Adam Smith‬</a>. I completed my PhD at University of Toronto and Vector Institute‬ where I was fortunate to be advised by  <a href="http://danroy.org/">‪Daniel M. Roy‬</a>. I also received my B.Sc. and M.Sc. degrees in Electrical Engineering from Sharif University of Technology.</p>

<p>During Summer and Fall 2022‪, I was a research intern at Google Brain (Differential Privacy Team) where I was lucky to be mentored by <a href="http://www.thomas-steinke.net/">Thomas Steinke</a>  and <a href="https://athakurta.squarespace.com/">Abhradeep Guha Thakurta‬</a>. 
I was also a research intern at <a href="https://www.elementai.com/">Element AI‬ (ServiceNow Research Lab)</a> in Winter 2019 and Fall 2020 where I had the opportunity to work with <a href="https://gkdz.org/">Gintare Karolina Dziugaite</a> in the Trustworthy AI Research Program. </p>

<p> Recognitions of my work include a <b>Best Paper Award at ICML 2024</b>, Simons Institute-UC Berkeley Research Fellowship, as well as several honors for graduate research excellence from University of Toronto, including the Henderson and Bassett Research Fellowship and the Viola Carless Smith Research Fellowship. Additionally, I was recognized as a top reviewer at NeurIPS in 2021 and 2023.</p>
  
<p> Outside my research activities, I enjoy playing and watching soccer, reading classic literature, and baking. </p>
  
<h2>Research Overview and Selected Papers </h2>
<p> My research focuses on the foundations and principled algorithm design for ML. More broadly, I am interested in statistical learning theory, statistics, and information theory. The central goal of my research is to address practical challenges in ML by developing tools and algorithms with rigorous theoretical guarantees that assess and ensure validity. This work is crucial for building trustworthy ML systems in high-stakes applications, where balancing responsible deployment with strong empirical performance is essential. Some of the questions I have been thinking about: When and how can models generalize beyond their training data? Under what conditions do they memorize sensitive information? And how can we preserve privacy while still learning effectively from sensitive data? </p>
<p><font color="purple"><u><b>Generalization in Machine Learning</b></u></font>:  </p>
<ul>
<li><p><a href="https://arxiv.org/abs/1911.02151">Information-Theoretic Generalization Bounds for SGLD via Data-Dependent Estimates</a> 
<br /> J. Negrea*, <b> M. Haghifam* </b>, G. K. Dziugaite, A. Khisti, D. M. Roy
<br /> Advances in Neural Information Processing Systems (NeurIPS), 2019 </p>
</li>
<li><p><a href="https://arxiv.org/abs/2111.05275">Towards a Unified Information&ndash;Theoretic Framework for Generalization</a>
<br /> <b> M. Haghifam </b>, G. K. Dziugaite, S. Moran,  D. M. Roy.
<br /> Advances in Neural Information Processing Systems (NeurIPS), 2021  <font color="blue"><b>(Spotlight, &lt;3% of submissions)</b></font>
</li>
</ul>
<p><font color="purple"><u><b>Memorization and Privacy Attacks</b></u></font>:  </p>
<ul>
<li><p><a href="https://arxiv.org/abs/2402.09327">Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization</a>
<br /> I. Attias, G. K. Dziugaite,<b> M. Haghifam </b>, R. Livni, D. M. Roy <img class="eq" src="eqs/1237943278-130.png" alt="(alphabeta)" style="vertical-align: -5px" />
<br /> International Conference on Machine Learning (ICML), 2024  <font color="blue"><b>(Oral, Best Paper Award: Top 10 of 10,000 submissions)</b></font>
</li>
<li><p><a href="https://arxiv.org/abs/2508.19458">The Sample Complexity of Membership Inference and Privacy Auditing</a>
<br /> <b>M. Haghifam </b>, A. Smith, J. Ullman <img class="eq" src="eqs/1237943278-130.png" alt="(alphabeta)" style="vertical-align: -5px" />
<br /> Pre-print, 2025 </p>
</li>
<li><p><a href="https://arxiv.org/abs/2502.17384">On Traceability in <img class="eq" src="eqs/1533415602-130.png" alt="ell_p" style="vertical-align: -6px" /> Stochastic Convex Optimization</a>
<br /> S. Voitovych*, <b>M. Haghifam* </b>, I. Attias, G. K. Dziugaite, R. Livni, D. M. Roy
<br /> Pre-print, 2025 
</ul>
<p><font color="purple"><u><b>Differential Privacy</b></u></font>: </p>
<ul>
<li><p><a href="https://arxiv.org/abs/2406.07407">Private Geometric Median</a>
<br /> <b> M. Haghifam </b>, T. Steinke, J. Ullman <img class="eq" src="eqs/1237943278-130.png" alt="(alphabeta)" style="vertical-align: -5px" />
<br /> Advances in Neural Information Processing Systems (NeurIPS), 2024 </p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.13209">Faster Differentially Private Convex Optimization via Second-Order Methods</a>
<br /> A. Ganesh, <b> M. Haghifam </b>, T. Steinke, A. Thakurta <img class="eq" src="eqs/1237943278-130.png" alt="(alphabeta)" style="vertical-align: -5px" />
<br /> Advances in Neural Information Processing Systems (NeurIPS) 2023</p>
</ul>
<h2>Contact Me!</h2>
<p>Feel free to reach out if you'd like to discuss research ideas. Also, I'm happy to offer guidance and support to those applying to graduate programs, especially individuals who might not typically have access to such assistance</p>
</td>
</tr>
</table>
</body>
</html>




















