<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Mahdi Haghifam </title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="cv.pdf">CV</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Mahdi Haghifam </h1>
</div>
<table class="imgtable"><tr><td>
<img src="photo/photo.jpg" alt="alt text" width="240px" height="203px" />&nbsp;</td>
<td align="left"><p>Postdoctoral Researcher,<br /> 
Khoury College of Computer Sciences at Northeastern University<br /> 
<a href="https://scholar.google.ca/citations?user=WADwBkkAAAAJ&amp;hl=en">[Google Scholar]</a> <a href="https://github.com/mhaghifam">[Github]</a> <a href="https://twitter.com/HaghifamMahdi">[Twitter]</a> <a href="https://www.linkedin.com/in/mahdi-haghifam/">[Linkedin]</a> <br />
Email(preferred): <a href="mailto:haghifam.mahdi@gmail.com">haghifam.mahdi@gmail.com</a> <br />
Email: <a href="mailto:m.haghifam@northeastern.edu">m.haghifam@northeastern.edu</a> <br />
<br />
<br /></p>
</td></tr></table>
<h2>About Me</h2>
<p>I am currently a  <a href="https://shorturl.at/kpquU">Distinguished Postdoctoral Researcher</a> at Khoury College of Computer Sciences at Northeastern University, fortuante to working with  <a href="https://jonathan-ullman.github.io/">Jonathan Ullman‬</a> and <a href="https://www.bu.edu/cs/profiles/adam-smith/">Adam Smith‬</a>.</p>
<p>In August 2023, I completed my PhD at University of Toronto and Vector Institute‬ where I was fortunate to be advised by  <a href="http://danroy.org/">‪Daniel M. Roy‬</a>. I also received my B.Sc. and M.Sc. degrees in Electrical Engineering from Sharif University of Technology.</p>
<p>My research focuses broadly on the foundations and methodologies for trustworthy machine learning. Recognitions of my work include a <b>Best Paper Award at ICML 2024</b>, the MITACS Accelerate Fellowship, as well as several honors for graduate research excellence, including the Henderson and Bassett Research Fellowship and the Viola Carless Smith Research Fellowship. Additionally, I was recognized as a top reviewer at NeurIPS in 2021 and 2023.</p>
<h2>Research </h2>
<p>I study how machine learning algorithms use information from their training set to learn a predictive model, with focus on developing theoretical foundations and practical algorithms for trustworthy ML systems. I am also interested in studying these questions in the context of large language models.</p>
<p><font color="purple"><u><b>Generalization in Machine Learning</b></u></font>: I develop tools to reason about and understand generalization in ML using information measures. </p>
<ul>
<li><p>Applications of information measures to reason about the generalization of practical algorithms (<a href="https://arxiv.org/abs/1911.02151">NeurIPS&rsquo;19</a>, <a href="https://arxiv.org/abs/2004.12983">NeurIPS&rsquo;20</a>). </p>
</li>
<li><p>Connections between generalization frameworks based on information measures and classical approaches in learning theory (such as VC theory and uniform stability) (<a href="https://arxiv.org/abs/2111.05275">NeurIPS&rsquo;21</a>, <a href="https://arxiv.org/abs/2212.13556">ALT&rsquo;23</a>)  </p>
</li>
</ul>
<p><font color="purple"><u><b>Membership Inference and Memorization</b></u></font>: Do accurate algorithms need to leak lots of information from their training data? </p>
<ul>
<li><p>Exact tradeoff between learning and membership inference in the fundamental setup of stochastic convex optimization (<a href="https://arxiv.org/abs/2402.09327">ICML&rsquo;24</a>, <a href="https://arxiv.org/abs/2502.17384">Arxiv&rsquo;25</a>)  </p>
</li>
<li><p>The statistical challenges of membership inference attacks (<a href="upcoming'25">upcoming'25</a>) </p>
</li>
</ul>
<p><font color="purple"><u><b>Differential Privacy</b></u></font>: How can algorithms learn from sensitive data without revealing private information? </p>
<ul>
<li><p>Practical algorithms that maintain worst-case privacy guarantees while adapting to dataset properties in order to achieve better performance   (<a href="https://arxiv.org/abs/2406.07407">NeurIPS&rsquo;24</a>).</p>
<li><p>Faster private optimization algorithms using second-order methods <a href="https://arxiv.org/abs/2305.13209">NeurIPS&rsquo;23</a>
</li>
</ul>
<h2>Internships and Research Visits</h2>
<p>During Summer and Fall 2022‪, I was a research intern at Google Brain (Differential Privacy Team) where I was extremely lucky to be mentored by <a href="http://www.thomas-steinke.net/">Thomas Steinke</a>  and <a href="https://athakurta.squarespace.com/">Abhradeep Guha Thakurta‬</a>. 
I was also a research intern at <a href="https://www.elementai.com/">Element AI‬ (ServiceNow Research Lab)</a> in Winter 2019 and Fall 2020 where I had the privilege of working with <a href="https://gkdz.org/">Gintare Karolina Dziugaite</a> in the Trustworthy AI Research Program. In early 2020, I had the opportunity to visit the <a href="https://www.ias.edu/">Institute of Advanced Studies</a> at Princeton as a visiting student for the special year program on optimization, statistics, and theoretical machine learning.</p>
<h2>Contact Me!</h2>
<p>Feel free to reach out if you'd like to discuss research ideas. Also, I'm happy to offer guidance and support to those applying to graduate programs, especially individuals who might not typically have access to such assistance</p>
</td>
</tr>
</table>
</body>
</html>
